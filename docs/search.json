[
  {
    "objectID": "tseries_nlVAR.html#overview",
    "href": "tseries_nlVAR.html#overview",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Overview",
    "text": "Overview\n\n\n\n1- Motivation\n      Typical problem\n      Data generation as DAG\n      Simulating the DAG\n2- Background and Notation\n      DAGs and matrix representation\n      Conditional Independence\n3- Vector Autoregressive (VAR) Models\n      A Toy example\n      Est VAR in high dimensional setting\n\n4- Network Inference\n      Thresholded LASSO\n      Generalizing to higher order\n5- Solution refinements\n      Model Tuning\n      Extended BIC\n      Dealing with uncertainty in obs window"
  },
  {
    "objectID": "tseries_nlVAR.html#motivation-1",
    "href": "tseries_nlVAR.html#motivation-1",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Motivation",
    "text": "Motivation\nGiven the case of an allergic reaction that is mediated through a measurable biomarker, consider the question re efficacy of treatment.\n\n\n\n\n\n\nflowchart TD\n  A((exposure)) --> B((biomarker))\n  C((treatment)) --> B\n  B --> D((reaction))"
  },
  {
    "objectID": "tseries_nlVAR.html#motivation-2",
    "href": "tseries_nlVAR.html#motivation-2",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Motivation",
    "text": "Motivation\nA slightly more realistic view of the data generation mechanism accounts (at least) for the first order temporal flow of the signal\n\nData Generation Viewed as DAG"
  },
  {
    "objectID": "tseries_nlVAR.html#motivation-3",
    "href": "tseries_nlVAR.html#motivation-3",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Motivation",
    "text": "Motivation\nIn this context, of primary interests are the effect of treatment \\(A_{trt}(t)\\) on the mediating biomarker \\(X_1(t)\\) and more importantly, the effect of \\(X_1(t)\\) on the outcome \\(Y_1(t)\\) (e.g. allergic reaction). Addressing the questions, could be viewed as a Network Inference problem.\n\nData Generation Viewed as DAG"
  },
  {
    "objectID": "tseries_nlVAR.html#simulation",
    "href": "tseries_nlVAR.html#simulation",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Simulation",
    "text": "Simulation\n\nTo make our mental model of the process even more explicitly defined, let’s consider the following DAG\n\n\n\nSimulation Dag t: baseline through 3"
  },
  {
    "objectID": "tseries_nlVAR.html#simulation-1",
    "href": "tseries_nlVAR.html#simulation-1",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Simulation",
    "text": "Simulation\nThe DAG can be implemented in simcausal as follows\n\nt_obs <- 1:30\nt.end <- length(t_obs)\n\nD <- DAG.empty() +\n  node(name = \"allergic\", distr = \"rbern\", prob = .5) +\n  node(name = \"responder\", distr = \"rbern\", prob = .5)+\n  node(name = \"unitnoise\", distr=\"rnorm\", mean=0, sd=.05) +\n  node(name = \"trt\", t = setdiff(1:t.end,c(2:4,8:10)), distr = \"rbern\", prob = 0)+\n  node(name = \"trt\", t = c(2:4,8:10), distr = \"rbern\", \n       prob = plogis( (x1bar[t-1] - 0.5) * 10)) +\n  node(name = \"pk\", t = 1, distr = \"rbern\", prob = 0)+\n  node(name = \"pk\", t = 2:t.end, distr = \"getPk\", pkm1 = pk[t-1], rx = trt[t])+\n  node(name = \"x1bar\", t = 1:t.end, distr = \"aveProfile\", time=t, pk = pk[t],\n       allergic=allergic, unitnoise = unitnoise) +\n  node(name = \"x2bar\", t = 1:t.end, distr = \"aveProfile2\", time=t,\n       allergic=allergic, unitnoise = unitnoise, x1bar = x1bar[t], \n       responder = responder) +\n  \n  node(name = \"rxn\", t = 1:t.end, distr = \"rbern\",\n       prob = plogis( (x2bar[t] - 0.65) * 30))\n  \n  obj_D <- set.DAG(D)"
  },
  {
    "objectID": "tseries_nlVAR.html#motivation-simulation",
    "href": "tseries_nlVAR.html#motivation-simulation",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Motivation: Simulation",
    "text": "Motivation: Simulation\n\nsim_xbar <- simcausal::sim(DAG = obj_D,n = 100)\nsim_xbar_long <- format_long(sim_xbar)"
  },
  {
    "objectID": "tseries_nlVAR.html#motivation-simulation-1",
    "href": "tseries_nlVAR.html#motivation-simulation-1",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Motivation: Simulation",
    "text": "Motivation: Simulation"
  },
  {
    "objectID": "tseries_nlVAR.html#background-and-notation-1",
    "href": "tseries_nlVAR.html#background-and-notation-1",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Background and Notation",
    "text": "Background and Notation\n\n\nNetwork: Nodes + Edges\n\n\nAdjacency matrix: not-directed\n\\[A_{0/1} = \\begin{bmatrix}\n0 & 1 & 1 & 0  \\\\\n1 & 0 & 1 & 0  \\\\\n1 & 1 & 0 & 1  \\\\\n0 & 0 & 1 & 0  \\\\\n\\end{bmatrix}\\]\nAdjacency matrix: directed \\[A = \\begin{bmatrix}\n0.0 & 0.5 & 2.0 & 0.0  \\\\\n0.0 & 0.0 & 1.0 & 0.0  \\\\\n0.0 & 0.0 & 0.0 & 0.0  \\\\\n0.0 & 0.0 & -1.0 & 0.0  \\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "tseries_nlVAR.html#background-and-notation-2",
    "href": "tseries_nlVAR.html#background-and-notation-2",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Background and Notation",
    "text": "Background and Notation\n\nData representation:\n\n\\(X^t\\): Observations (data) at time \\(t\\)\n\\(X^t \\in R^{nxp}\\): \\(X\\) contains \\(n\\) observations on \\(p\\) nodes at time \\(t\\)\n\nNote on conditional independence\nConsider\n\n\n\n\nflowchart LR\n  x1((A)) --> x2((B))\n  x2 --> x3((C))\n\n\n\n\n\n\n\n\nFor this simple graph, conditional independence can be stated as\n\\[C \\not\\!\\perp\\!\\!\\!\\perp A | B\\] As the number of nodes, \\(p\\), grows, establishing conditional independence globally becomes challenging and many network inference algorithms rely on some variation of pairwise scoring between nodes, which is much more tractable. However, those algorithms are more prone to false edge discovery."
  },
  {
    "objectID": "tseries_nlVAR.html#establishing-conditional-independence-through-regression",
    "href": "tseries_nlVAR.html#establishing-conditional-independence-through-regression",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Establishing Conditional Independence Through Regression",
    "text": "Establishing Conditional Independence Through Regression\n\n\n\nxx <- sim_xbar_long %>% select(ID, t, x1) %>% \n    tidyr::pivot_wider(names_from = t,values_from = x1, names_prefix = \"t_\") \n\nxx %>% select(t_1, t_2, t_3) %>% plot\n\n\n\n\n\n\nmod <- lm(t_3 ~ t_2, data = xx)\nplot(mod$residuals, xx$t_1)"
  },
  {
    "objectID": "tseries_nlVAR.html#vector-autoregressive-var-models-1",
    "href": "tseries_nlVAR.html#vector-autoregressive-var-models-1",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Vector Autoregressive (VAR) Models",
    "text": "Vector Autoregressive (VAR) Models\n\n\n\n\n\n\nNetwork VAR Model: General Idea\n\n\nStates observed over the past \\(d\\) time points are predictive of the current observed state, \\(X^T\\)\n\n\n\n\n\nExample: a network with \\(p=5\\) and \\(d = 2\\) \nTask: estimate \\(d\\) and \\(A^t\\) given \\(X^t\\), \\(t \\in \\{1, ..., T\\}\\)\n\n\\[X^T = X^{T-1}A^1 + ... + X^{T-d}A^d + \\epsilon^T, ~ \\epsilon^T \\sim\n\\mathcal{MN}(0_{nxp},\\sigma^2 I_n, \\sigma^2 I_p) ~~ (1)\\]"
  },
  {
    "objectID": "tseries_nlVAR.html#estimating-ats-and-d",
    "href": "tseries_nlVAR.html#estimating-ats-and-d",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Estimating \\(A^t\\)s and \\(d\\)",
    "text": "Estimating \\(A^t\\)s and \\(d\\)\n\nCombining \\(X^T \\rightarrow \\mathcal{X}\\) and stacking \\(A^t \\rightarrow \\mathcal{A}\\):\n\n\\[X^T = \\mathcal{X} \\mathcal{A} + \\epsilon^T, ~\\epsilon^T \\sim\n\\mathcal{MN}(0_{nxp},\\sigma^2 I_n, \\sigma^2 I_p) ~~ (2)\\]\n\nEstimating \\(A^t\\)s amounts to regressing columns of \\(X^T\\) on one at a time\nObserving that\n\n  1. \\(\\mathcal{X}\\) is high dimensional (\\(n \\ll p\\)) and that\n  2. \\(A^t\\) are expected to be sparse\n  regression involves both variable selection (which nodes are connected) and estimation (quantification of connection strength). To this end, we can impose a \\(\\ell_1\\) type penalty to the regression. To estimate \\(d\\) we could then use thresholded LASSO (Shojaie et al)."
  },
  {
    "objectID": "tseries_nlVAR.html#estimating-ats-and-d-using-thresholded-lasso",
    "href": "tseries_nlVAR.html#estimating-ats-and-d-using-thresholded-lasso",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Estimating \\(A^t\\)s and \\(d\\) Using Thresholded LASSO",
    "text": "Estimating \\(A^t\\)s and \\(d\\) Using Thresholded LASSO\n\nA 3-Step process for estimating \\(A^t\\)s and d by\n\nEstimate \\(\\bar{A^t}\\) by fit eq. 2 for each column \\(j\\) of \\(X^T\\)\n\n\\[\\underset{{\\alpha_0 \\in \\mathbb{R}^{1},\\mathcal{A}_{.j} \\in \\mathbb{R}^{q} }}{\\arg\\min} n^{-1}\n\\| X_{.j}^T  - \\alpha_0 - \\mathcal{X}\\mathcal{A}_{.j}  \\|_2^2\n+ \\lambda \\|\\mathcal{A}_{.j}\\|_1~~(3)\\]\n\nThreshold \\(\\bar{A^t}\\) to get the thresholded est. adj matrix, \\(\\hat{A^t}\\)\n\n\\[\\hat{A_{ij}^t} = \\bar{A_{ij}^t} 1_{\\{ \\|\\bar{A}^t\\|_0 < \\frac{p^2 \\beta}{T-1} \\text{ and }  |\\bar{A_{ij}^t}| < \\tau \\}}~~(4)\\]\n\nEstimate the order of the VAR model, \\(\\hat{d}\\), as follows:\n\n\\[\\hat{d} = \\max_{t} \\{t: \\|\\hat{A}^{T-t}\\|_0 < \\frac{p^2 \\beta}{T-1} \\}~~(5)\\] where \\(\\beta\\) is the desired rate of type II error for detecting an edge between two nodes. \\(\\lambda\\) and \\(\\tau\\) are tuning parameters with default values suggested by Shojaie et al."
  },
  {
    "objectID": "tseries_nlVAR.html#estimating-ats-and-d-a-toy-example",
    "href": "tseries_nlVAR.html#estimating-ats-and-d-a-toy-example",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Estimating \\(A^t\\)s and \\(d\\): A Toy Example",
    "text": "Estimating \\(A^t\\)s and \\(d\\): A Toy Example\n\nThe 3-steps procedure for a network of size 3 observed over 3 time-points.\n\nGet the adj. mat. estimates \\([\\bar{A^1}~ \\bar{A^2}~\\bar{A^3}]\\) via LASSO regression:\n\n\\[\\begin{bmatrix}\n0.00 & 0.00 & 1.00 ~~&~~ 1.00 & 0.00 & 1.00 ~~&~~ 0.10 & 2.00 & 0.90\\\\\n0.00 & 0.00 & 0.00 ~~&~~ 0.00 & 5.00 & 0.00 ~~&~~ 0.00 & 0.00 & 0.00\\\\\n0.00 & 0.00 & 0.00 ~~&~~ 0.35 & 0.32 & 0.09 ~~&~~ 5.00 & 0.20 & 0.15\\\\\n\\end{bmatrix}\\]\n\nThreshold using eq. 4 with \\(\\frac{p^2 \\beta}{T-1} = 2\\) and \\(\\tau = 1\\)\n\n\\[\\begin{bmatrix}\n0 & 0 & 0 ~~&~~ 1 & 0 & 1 ~~&~~ 0 & 2 & 0\\\\\n0 & 0 & 0 ~~&~~ 0 & 5 & 0 ~~&~~ 0 & 0 & 0\\\\\n0 & 0 & 0 ~~&~~ 0 & 0 & 0 ~~&~~ 5 & 0 & 0\\\\\n\\end{bmatrix}\\]\n\nEstimate the order: \\(\\hat{d} = \\max_{t} \\{t: \\|\\hat{A}^t\\|_0 < 2 \\} = 2\\)"
  },
  {
    "objectID": "tseries_nlVAR.html#generalizing-the-model-capturing-higher-order-relations",
    "href": "tseries_nlVAR.html#generalizing-the-model-capturing-higher-order-relations",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Generalizing the Model: Capturing Higher Order Relations",
    "text": "Generalizing the Model: Capturing Higher Order Relations\n\n\nVAR model of eq. 1 estimates the linear trends in the inter-relations of nodes\nTo capture higher order relations, we transform observations using a smoother (e.g. B-spline)\n\n\\[X^T = Y^{T-1}B^1 + \\cdots + Y^{T-d}B^d +\\epsilon^T, \\epsilon^T \\sim \\mathcal{MN}(\\bf{0}_{\\textrm{n} \\times \\textrm{p}},~\\sigma^2I_n ,\\sigma^2I_p)~~(6) \\] Here, \\(Y^t = [f(X_{.1}^t), \\cdots, f(X_{.p}^t)]\\in \\mathbb{R}^{\\textrm{n}\\times \\textrm{u}}\\)\n\nWe penalize the coefficients by groups corresponding to expansion of a single variable using group LASSO\n\n\\[\\underset{\\beta_0 \\in \\mathbb{R},\\mathcal{B}_{.j} \\in \\mathbb{R}^{s} } {\\arg\\min}  \\frac{1}{2n}\n\\| X_{.j}^T  - \\beta_0 - \\mathcal{Y} \\mathcal{B}_{.j}  \\|_2^2\n+ \\lambda \\sum\\limits_{i=1}^p \\sqrt{k} \\|\\mathcal{B}_{g_ij}\\|_1, ~~ (7)\\]\n\nThresholding is done analogous to the linear case but using \\(\\ell^2\\) norm of est. coefficient groups."
  },
  {
    "objectID": "tseries_nlVAR.html#model-tuning-regression-penalty-term-lambda",
    "href": "tseries_nlVAR.html#model-tuning-regression-penalty-term-lambda",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Model Tuning: Regression Penalty Term \\(\\lambda\\)",
    "text": "Model Tuning: Regression Penalty Term \\(\\lambda\\)\n\n\nNetwork inference: filter out spurious associations, keep true conditional dependencies\nthe larger the penalty, \\(\\lambda\\), the more conservative the filter\nTo pick the right level of filtering, there are multiple choices including general cross validation as well as likelihood-based scores, such as Schwarz Bayesian Information Criteria (SBIC) score.\n\n\\[\\textrm{SBIC} = \\: \\log \\: \\mathcal{L}(\\theta_{MLE}|x) - \\frac{DOF_{MLE}}{2} \\: \\log \\: n ~~ (8)\\]   where DOF is the degrees of freedom of MLE model: \\(\\lVert\\theta_{MLE}\\rVert_0\\)\n\nIn the context of linear regression model, \\(lm\\), and assumption of normality, BIC (which is often represented as SBIC scaled by -2) is simplified (up to a constant) as:\n\n\\[\\textrm{BIC} = \\: n \\: \\log \\: \\frac{RSS}{n} + DOF_{lm} \\: \\log \\: n ~~ (9)\\]"
  },
  {
    "objectID": "tseries_nlVAR.html#model-tuning-bics-flat-prior-assumption",
    "href": "tseries_nlVAR.html#model-tuning-bics-flat-prior-assumption",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Model Tuning: BIC’s flat Prior Assumption",
    "text": "Model Tuning: BIC’s flat Prior Assumption\n\n\nIn high-dimensional settings, BIC is too liberal in model selection\nFlat Prior Assumption of Schwarz’s BIC: all possible models equally likely given that we have no information on the network\nConsider the case where there are 1000 parameters (edges) that could be included in the model (network).\n\n1 param model choices: \\(\\binom{1000}{1} = 1000\\)\n2 param model choices: \\(\\binom{1000}{2} = 1000 * 999/2\\)\n\\(\\rightarrow\\) two parameter model are 999/2 more likely than single parameter model\n\nExtended BIC, proposed by Chen and Chen, sets the prior to be flat for model groups of same number of parameters."
  },
  {
    "objectID": "tseries_nlVAR.html#model-tuning-extended-bic-with-widehattextrmdof_textrmgrplasso",
    "href": "tseries_nlVAR.html#model-tuning-extended-bic-with-widehattextrmdof_textrmgrplasso",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Model Tuning: Extended BIC with \\(\\widehat{\\textrm{DOF}}_{\\textrm{grplasso}}\\)",
    "text": "Model Tuning: Extended BIC with \\(\\widehat{\\textrm{DOF}}_{\\textrm{grplasso}}\\)\n\n\nExtended BIC (EBIC) is given by\n\n\\[\\textrm{EBIC} = \\textrm{BIC} + 2~\\textrm{DOF}~\\gamma~ log~p,~ \\text{where}~  0\\leq \\gamma ~~(10)\\]\n      DOF is the degrees of freedom and \\(\\gamma\\) is a tuning parameter that can be increased as p grows larger\n      relative to n. At \\(\\gamma = 0\\), EBIC reduces to BIC\n\nNote: DOF relates to the number of model parameter. However, the true number of parameters (edges) to estimated itself is unknown \\(\\rightarrow\\) We need an estimator for DOF.\nFor group LASSO, we can use Vaiter et al. est. DOF\n\n\\[\\widehat{\\textrm{DOF}}_{\\textrm{grplasso}} =  \\textrm{Trace}[X_a(X_a'X_a +\n\\lambda \\mathscr{N}(\\hat{\\beta_a})\\odot\n(I - P_{\\hat{\\beta_a}}))^{-1}X_a'] ~~(11)\\]\n\nwhere \\(\\lambda\\) is the group LASSO penalty, \\(X_a\\) and \\(\\hat{\\beta_a}\\) correspond to ‘selected` groups of variables. \\(P_{\\hat{\\beta_a}}\\) is a block diagonal matrix, each block the projection matrix of one of the ’selected’ variables. \\(\\mathscr{N}(\\hat{\\beta_a})\\) normalizes each block by its \\(\\ell^2\\) norm of \\(\\hat{\\beta}_{\\textrm{group}}\\)."
  },
  {
    "objectID": "tseries_nlVAR.html#catching-the-signal-in-the-time-course",
    "href": "tseries_nlVAR.html#catching-the-signal-in-the-time-course",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Catching the ‘Signal’ in the Time Course",
    "text": "Catching the ‘Signal’ in the Time Course\n\nConditional dependencies (edges) between nodes are learned from observing nonde-node co-variations\nAs a signal propagates through network, different sets of nodes exhibit their co-variations maximally at different time windows\nDuring which time window should these co-variations be estimated?"
  },
  {
    "objectID": "tseries_nlVAR.html#bayesian-model-averaging-bma",
    "href": "tseries_nlVAR.html#bayesian-model-averaging-bma",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Bayesian Model Averaging (BMA)",
    "text": "Bayesian Model Averaging (BMA)\n\n\nFor \\(\\forall~\\mathcal{X}_t = [X^{1}, ..., X^{t}]\\), where \\(t \\in \\{2,~...,~ T\\}\\) infer network: \\(M_t\\) and the corresponding graph \\(G_t\\)\nScore each inferred network, \\(M_t\\), using BIC 1,\n\n\\[w_i = \\frac{\\exp(-\\frac{1}{2} \\textrm{BIC}_{i})}{\\sum\\limits_{j=1}^{T-1}  \n\\exp(-\\frac{1}{2} \\textrm{BIC}_j) } ~~(12)\\]\n\nPerform a weighted average of the est. graph\n\n\\[\\hat{\\mathcal{G}} = \\sum\\limits_{t=1}^{T-1}w_t{\\hat{G}_t} ~~(13)\\]\n\nSelect a thresholding \\(\\mathcal{p} \\in [0~1]\\) and define thresholded BMA graph , \\(\\breve{\\mathcal{G}}\\) such that: \\(\\breve{\\mathcal{G}}_{ij} = \\hat{\\mathcal{G}_{ij}} 1_{[\\hat{\\mathcal{G}_{ij}} > \\mathcal{p}]}\\)\n\n\nBIC is used generically to refer to EBIC or BIC"
  },
  {
    "objectID": "tseries_nlVAR.html#summary-and-future-work",
    "href": "tseries_nlVAR.html#summary-and-future-work",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Summary and Future Work",
    "text": "Summary and Future Work\n\nThe framework described has the following favorable properties\n\nWith some mild assumptions, it can infer network topology based on conditional independence\nIt is well-suited high-dimensional data, assuming sparsity of network\nIt can handle higher order trajectories\nIt expand to simultaneously process evolution of multiple biomarkers\nIt can handle experimental fixing of nodes\nIt relies on well-established and computationally efficient regression machinery\n\nFeature work\n\nShowing efficiency using simulated data\nExpansion to censored data\nComputational parallelization\nTotal regression (as opposed to least squared)"
  }
]