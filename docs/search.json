[
  {
    "objectID": "tseries_nlVAR.html#overview",
    "href": "tseries_nlVAR.html#overview",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Overview",
    "text": "Overview\n\n\n1- Motivation\n      Typical problem\n      Data generation as DAG\n2- Background and Notation\n      DAGs and matrix representation\n      Representing data\n3- Vector Autoregressive (VAR) Models\n      A Toy example\n      Est VAR in high dimensional setting"
  },
  {
    "objectID": "tseries_nlVAR.html#motivation-1",
    "href": "tseries_nlVAR.html#motivation-1",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Motivation",
    "text": "Motivation\n\nData Generation Mechanism"
  },
  {
    "objectID": "tseries_nlVAR.html#motivation-2",
    "href": "tseries_nlVAR.html#motivation-2",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Motivation",
    "text": "Motivation\n\nData Generation Mechanism as DAG"
  },
  {
    "objectID": "tseries_nlVAR.html#motivation-3",
    "href": "tseries_nlVAR.html#motivation-3",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Motivation",
    "text": "Motivation\n\nData Generation Mechanism as DAG"
  },
  {
    "objectID": "tseries_nlVAR.html#background-and-notation-1",
    "href": "tseries_nlVAR.html#background-and-notation-1",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Background and Notation",
    "text": "Background and Notation\n\n\nGraph: Nodes + Edges\n\n\n\nExample\n\n\n\nAdjacency matrix: not-directed\n\\[A_{0/1} = \\begin{bmatrix}\n0 & 1 & 1 & 0  \\\\\n1 & 0 & 1 & 0  \\\\\n1 & 1 & 0 & 1  \\\\\n0 & 0 & 1 & 0  \\\\\n\\end{bmatrix}\\]\nAdjacency matrix: directed \\[A = \\begin{bmatrix}\n0.0 & 0.5 & 2.0 & 0.0  \\\\\n0.0 & 0.0 & 1.0 & 0.0  \\\\\n0.0 & 0.0 & 0.0 & 0.0  \\\\\n0.0 & 0.0 & -1.0 & 0.0  \\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "tseries_nlVAR.html#background-and-notation-2",
    "href": "tseries_nlVAR.html#background-and-notation-2",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Background and Notation",
    "text": "Background and Notation\nData representation:\n\n\\(X^t\\): Observations (data) at time \\(t\\)\n\\(X^t \\in R^{nxp}\\): \\(X\\) contains \\(n\\) observations on \\(p\\) nodes at time \\(t\\)"
  },
  {
    "objectID": "tseries_nlVAR.html#vector-autoregressive-var-models-1",
    "href": "tseries_nlVAR.html#vector-autoregressive-var-models-1",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Vector Autoregressive (VAR) Models",
    "text": "Vector Autoregressive (VAR) Models\n\n\n\n\n\n\nNetwork VAR Model: General Idea\n\n\nStates observed over the past \\(d\\) time points are predictive of the current observed state, \\(X^T\\)\n\n\n\n\n\nExample: a network with \\(p=5\\) and \\(d = 2\\) \nTask: estimate \\(d\\) and \\(A^t\\) given \\(X^t\\), \\(t \\in \\{1, ..., T\\}\\)\n\n\\[X^T = X^{T-1}A^1 + ... + X^{T-d}A^d + \\epsilon^T, ~ \\epsilon^T \\sim\n\\mathcal{MN}(0_{nxp},\\sigma^2 I_n, \\sigma^2 I_p) ~~ (1)\\]"
  },
  {
    "objectID": "tseries_nlVAR.html#estimating-ats-and-d",
    "href": "tseries_nlVAR.html#estimating-ats-and-d",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Estimating \\(A^t\\)s and \\(d\\)",
    "text": "Estimating \\(A^t\\)s and \\(d\\)\n\nCombining \\(X^T \\rightarrow \\mathcal{X}\\) and stacking \\(A^t \\rightarrow \\mathcal{A}\\):\n\n\\[X^T = \\mathcal{X} \\mathcal{A} + \\epsilon^T, ~\\epsilon^T \\sim\n\\mathcal{MN}(0_{nxp},\\sigma^2 I_n, \\sigma^2 I_p) ~~ (2)\\]\n\nEstimating \\(A^t\\)s amounts to regressing columns of \\(X^T\\) on one at a time\nObserving that\n\n  1. \\(\\mathcal{X}\\) is high dimensional (\\(n \\ll p\\)) and that\n  2. \\(A^t\\) are expected to be sparse\n  regression involves both variable selection (which nodes are connected) and estimation (quantification of connection strength). To this end, we can impose a \\(\\ell_1\\) type penalty to the regression. To estimate \\(d\\) we could then use thresholded LASSO (Shojaie et al)."
  },
  {
    "objectID": "tseries_nlVAR.html#estimating-ats-and-d-using-thresholded-lasso",
    "href": "tseries_nlVAR.html#estimating-ats-and-d-using-thresholded-lasso",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Estimating \\(A^t\\)s and \\(d\\) Using Thresholded LASSO",
    "text": "Estimating \\(A^t\\)s and \\(d\\) Using Thresholded LASSO\n\nA 3-Step process for estimating \\(A^t\\)s and d by\n\nEstimate \\(\\bar{A^t}\\) by fit eq. 2 for each column \\(j\\) of \\(X^T\\)\n\n\\[\\underset{{\\alpha_0 \\in \\mathbb{R}^{1},\\mathcal{A}_{.j} \\in \\mathbb{R}^{q} }}{\\arg\\min} n^{-1}\n\\| X_{.j}^T  - \\alpha_0 - \\mathcal{X}\\mathcal{A}_{.j}  \\|_2^2\n+ \\lambda \\|\\mathcal{A}_{.j}\\|_1~~(3)\\]\n\nThreshold \\(\\bar{A^t}\\) to get the thresholded est. adj matrix, \\(\\hat{A^t}\\)\n\n\\[\\hat{A_{ij}^t} = \\bar{A_{ij}^t} 1_{\\{ \\|\\bar{A}^t\\|_0 < \\frac{p^2 \\beta}{T-1} \\text{ and }  |\\bar{A_{ij}^t}| < \\tau \\}}~~(4)\\]\n\nEstimate the order of teh VAR model, \\(\\hat{d}\\), as follows:\n\n\\[\\hat{d} = \\max_{t} \\{t: \\|\\hat{A}^t\\|_0 < \\frac{p^2 \\beta}{T-1} \\}~~(5)\\] where \\(\\beta\\) is the desired rate of type II error for detecting an edge between two nodes. \\(\\lambda\\) and \\(\\tau\\) are tuning parameters with default values suggested by Shojaie et al."
  },
  {
    "objectID": "tseries_nlVAR.html#estimating-ats-and-d-a-toy-example",
    "href": "tseries_nlVAR.html#estimating-ats-and-d-a-toy-example",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Estimating \\(A^t\\)s and \\(d\\): A Toy Example",
    "text": "Estimating \\(A^t\\)s and \\(d\\): A Toy Example\n\nThe 3-steps procedure for a network of size 3 observed over 3 time-points.\n\nGet the adj. mat. estimates \\([\\bar{A^1}~ \\bar{A^2}~\\bar{A^3}]\\) via LASSO regression:\n\n\\[\\begin{bmatrix}\n0.00 & 0.00 & 1.00 ~~&~~ 1.00 & 0.00 & 1.00 ~~&~~ 0.10 & 2.00 & 0.90\\\\\n0.00 & 0.00 & 0.00 ~~&~~ 0.00 & 5.00 & 0.00 ~~&~~ 0.00 & 0.00 & 0.00\\\\\n0.00 & 0.00 & 0.00 ~~&~~ 0.35 & 0.32 & 0.09 ~~&~~ 5.00 & 0.20 & 0.15\\\\\n\\end{bmatrix}\\]\n\nThreshold using eq. 4 with \\(\\frac{p^2 \\beta}{T-1} = 2\\) and \\(\\tau = 1\\)\n\n\\[\\begin{bmatrix}\n0 & 0 & 0 ~~&~~ 1 & 0 & 1 ~~&~~ 0 & 2 & 0\\\\\n0 & 0 & 0 ~~&~~ 0 & 5 & 0 ~~&~~ 0 & 0 & 0\\\\\n0 & 0 & 0 ~~&~~ 0 & 0 & 0 ~~&~~ 5 & 0 & 0\\\\\n\\end{bmatrix}\\]\n\nEstimate the order: \\(\\hat{d} = \\max_{t} \\{t: \\|\\hat{A}^t\\|_0 < 2 \\} = 2\\)"
  },
  {
    "objectID": "tseries_nlVAR.html#generalizing-the-model-capturing-higher-order-relations",
    "href": "tseries_nlVAR.html#generalizing-the-model-capturing-higher-order-relations",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Generalizing the Model: Capturing Higher Order Relations",
    "text": "Generalizing the Model: Capturing Higher Order Relations\n\n\nVAR model of eq. 1 estimates the linear trends in the inter-relations of nodes\nTo capture higher order relations, we transform observations using a smoother (e.g. B-spline)\n\n\\[X^T = Y^{T-1}B^1 + \\cdots + Y^{T-d}B^d +\\epsilon^T, \\epsilon^T \\sim \\mathcal{MN}(\\bf{0}_{\\textrm{n} \\times \\textrm{p}},~\\sigma^2I_n ,\\sigma^2I_p)~~(6) \\] Here, \\(Y^t = [f(X_{.1}^t), \\cdots, f(X_{.p}^t)]\\in \\mathbb{R}^{\\textrm{n}\\times \\textrm{u}}\\)\n\nWe penalize the coefficients by groups corresponding to expansion of a single variable using group LASSO\n\n\\[\\underset{\\beta_0 \\in \\mathbb{R},\\mathcal{B}_{.j} \\in \\mathbb{R}^{s} } {\\arg\\min}  \\frac{1}{2n}\n\\| X_{.j}^T  - \\beta_0 - \\mathcal{Y} \\mathcal{B}_{.j}  \\|_2^2\n+ \\lambda \\sum\\limits_{i=1}^p \\sqrt{k} \\|\\mathcal{B}_{g_ij}\\|_1, ~~ (7)\\]\n\nThresholding is done analogous to the linear case but using \\(\\ell^2\\) norm of est. coefficient groups."
  },
  {
    "objectID": "tseries_nlVAR.html#model-tuning-regression-penalty-term-lambda",
    "href": "tseries_nlVAR.html#model-tuning-regression-penalty-term-lambda",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Model Tuning: Regression Penalty Term \\(\\lambda\\)",
    "text": "Model Tuning: Regression Penalty Term \\(\\lambda\\)\n\n\nNetwork inference: filter out spurious associations, keep true conditional dependencies\nthe larger the penalty, \\(\\lambda\\), the more conservative the filter\nTo pick the right level of filtering, there are multiple choices including general cross validation as well as likelihood-based scores, such as Schwarz Bayesian Information Criteria (SBIC) score.\n\n\\[\\textrm{SBIC} = \\: \\log \\: \\mathcal{L}(\\theta_{MLE}|x) - \\frac{DOF_{MLE}}{2} \\: \\log \\: n ~~ (7)\\]   where DOF is the degrees of freedom of MLE model: \\(\\lVert\\theta_{MLE}\\rVert_0\\)\n\nIn the context of linear regression model, \\(lm\\), and assumption of normality, BIC (which is often represented as SBIC scaled by -2) is simplified (up to a constant) as:\n\n\\[\\textrm{BIC} = \\: n \\: \\log \\: \\frac{RSS}{n} + DOF_{lm} \\: \\log \\: n ~~ (8)\\]"
  },
  {
    "objectID": "tseries_nlVAR.html#model-tuning-bics-flat-prior-assumption",
    "href": "tseries_nlVAR.html#model-tuning-bics-flat-prior-assumption",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Model Tuning: BIC’s flat Prior Assumption",
    "text": "Model Tuning: BIC’s flat Prior Assumption\n\n\nIn high-dimensional settings, BIC is too liberal in model selection\nFlat Prior Assumption of Schwarz’s BIC: all possible models equally likely given that we have no information on the network\nConsider the case where there are 1000 parameters (edges) that could be included in the model (network).\n\n1 param model choices: \\(\\binom{1000}{1} = 1000\\)\n2 param model choices: \\(\\binom{1000}{2} = 1000 * 999/2\\)\n\\(\\rightarrow\\) two parameter model are 999/2 more likely than single parameter model\n\nExtended BIC, proposed by Chen and Chen, sets the prior to be flat for model groups of same number of parameters."
  },
  {
    "objectID": "tseries_nlVAR.html#model-tuning-extended-bic-with-widehattextrmdof_textrmgrplasso",
    "href": "tseries_nlVAR.html#model-tuning-extended-bic-with-widehattextrmdof_textrmgrplasso",
    "title": "Time-series Inference Using a Non-linear Vector Autoregressive Model",
    "section": "Model Tuning: Extended BIC with \\(\\widehat{\\textrm{DOF}}_{\\textrm{grplasso}}\\)",
    "text": "Model Tuning: Extended BIC with \\(\\widehat{\\textrm{DOF}}_{\\textrm{grplasso}}\\)\n\n\nExtended BIC (EBIC) is given by\n\n\\[\\textrm{EBIC} = \\textrm{BIC} + 2~\\textrm{DOF}~\\gamma~ log~p,~ \\text{where}~  0\\leq \\gamma ~~(9)\\]\n      DOF is the degrees of freedom and \\(\\gamma\\) is a tuning parameter that can be increased as p grows larger\n      relative to n. At \\(\\gamma = 0\\), EBIC reduces to BIC\n\nNote: DOF relates to the number of model parameter. However, the true number of parameters (edges) to estimated itself is unknown \\(\\rightarrow\\) We need an estimator for DOF.\nFor group LASSO, we can use Vaiter et al. est. DOF\n\n\\[\\widehat{\\textrm{DOF}}_{\\textrm{grplasso}} =  \\textrm{Trace}[X_a(X_a'X_a +\n\\lambda \\mathscr{N}(\\hat{\\beta_a})\\odot\n(I - P_{\\hat{\\beta_a}}))^{-1}X_a'] ~~(10)\\]\n\nwhere \\(\\lambda\\) is the group LASSO penalty, \\(X_a\\) and \\(\\hat{\\beta_a}\\) correspond to ‘selected` groups of variables. \\(P_{\\hat{\\beta_a}}\\) is a block diagonal matrix, each block the projection matrix of one of the ’selected’ variables. \\(\\mathscr{N}(\\hat{\\beta_a})\\) normalizes each block by its \\(\\ell^2\\) norm of \\(\\hat{\\beta}_{\\textrm{group}}\\)."
  }
]