---
title: "Time-series Inference Using a Non-linear Vector Autoregressive Model"
author: "Afshin Mashadi-Hossein"
date: "`r Sys.Date()`"
---

```{r setup, include=F}
source("./R/global.R")
```

## Overview

::: {style="font-size:24px"}
::: columns
::: {.column width="50%"}
### 1- Motivation

|       [Typical problem](#2)
|       [Data generation as DAG](#3)
|       [Simulating the DAG](#5)

### 2- Background and Notation

|       [DAGs and matrix representation](#9)
|       [Conditional Independence](#10)

### 3- Vector Autoregressive (VAR) Models

|       [A Toy example](#11)
|       [Est VAR in high dimensional setting](#8)
:::

::: {.column width="50%"}
### 4- Network Inference

|       [Thresholded LASSO](#13)
|       [Generalizing to higher order](#15)

### 5- Solution refinements

|       [Model Tuning](#15)
|       [Extended BIC](#16)
|       [Dealing with uncertainty in obs window](#17)
:::
:::
:::

## Motivation

::: {style="font-size:24px"}
Given the case of an allergic reaction that is mediated through a measurable biomarker, consider the question re efficacy of treatment.

::: columns
::: {.column width="30%"}
```{mermaid}
flowchart TD
  A((exposure)) --> B((biomarker))
  C((treatment)) --> B
  B --> D((reaction))
```
:::

::: {.column width="70%"}
![](images/data_motivation.png)
:::
:::
:::

## Motivation

::: {style="font-size:24px"}
A slightly more representative view of the data generation mechanism (than the simplistic DAG earlier) accounts (at least) for the first order temporal *flow* of the signal

![Data Generation Viewed as DAG](images/dag_motivation.png)
:::

## Viewing the Question as a Network Inference Task

In this context, of primary interests are the effect of treatment, $A_{trt}(t)$, on the mediating biomarker, $X_1(t)$, and more importantly, the effect of $X_1(t)$ on the outcome, $Y_1(t)$, (e.g. allergic reaction). Addressing these questions, could be viewed as a **Network Inference** problem.

![Data Generation Viewed as DAG](images/dag_motivation_highlighted.png)

## Specifying the Network

::: {style="font-size:24px"}
To make our mental model of the process even more explicitly defined, let's consider the following DAG

![Simulation Dag t: baseline through 3](images/sim_dag.png)
:::

## Coding the Network

The DAG can be implemented in `simcausal` as follows

```{r dag, message=FALSE, echo=TRUE}
t_obs <- 1:30
t.end <- length(t_obs)

D <- DAG.empty() +
  node(name = "allergic", distr = "rbern", prob = .5) +
  node(name = "responder", distr = "rbern", prob = .5)+
  node(name = "unitnoise", distr="rnorm", mean=0, sd=.05) +
  node(name = "trt", t = setdiff(1:t.end,c(2:4,8:10)), distr = "rbern", prob = 0)+
  node(name = "trt", t = c(2:4,8:10), distr = "rbern", 
       prob = plogis( (x1bar[t-1] - 0.5) * 10)) +
  node(name = "pk", t = 1, distr = "rbern", prob = 0)+
  node(name = "pk", t = 2:t.end, distr = "getPk", pkm1 = pk[t-1], rx = trt[t])+
  node(name = "x1bar", t = 1:t.end, distr = "aveProfile", time=t, pk = pk[t],
       allergic=allergic, unitnoise = unitnoise) +
  node(name = "x2bar", t = 1:t.end, distr = "aveProfile2", time=t,
       allergic=allergic, unitnoise = unitnoise, x1bar = x1bar[t], 
       responder = responder) +
  
  node(name = "rxn", t = 1:t.end, distr = "rbern",
       prob = plogis( (x2bar[t] - 0.65) * 30))
  
  obj_D <- set.DAG(D)
```

## Simulating the Network

```{r, echo=TRUE}
set.seed(3)
sim_xbar <- simcausal::sim(DAG = obj_D,n = 100)
sim_xbar_long <- format_long(sim_xbar)
```

```{r}
g <- sim_xbar_long %>% ggplot(aes(x = t, y=x1, colour = allergic, text = ID))+
  geom_line(aes(group = ID), alpha = .2)+
  geom_point(data = sim_xbar_long %>% filter(rxn == 1), size = 2)+
  geom_point(data = sim_xbar_long %>% filter(trt == 1), color = "black",size = 1)+
  geom_point(data = sim_xbar_long %>% filter(trt == 1), color = "white",size = .5)+
  facet_grid(treated_early ~ responder,labeller = label_both)+
  
  # ylab(expression(X[1](t)))+
  ylab("X1")+
  ggsci::scale_color_jama() +
  theme_bw()

plotly::ggplotly(g)
```

## Simulating the Network

```{r, echo=F}
g <- sim_xbar_long %>% 
  mutate(rxn = if_else(rxn == 0, "N","Y")) %>%
  mutate(trt = if_else(trt == 0, "N","Y")) %>%
  ggplot(aes(x = rxn, y = x1, color = trt))+
  geom_boxplot(outlier.shape = NA, alpha = .2)+
  geom_jitter(alpha = .3, aes(shape = trt))+
  facet_wrap(treated_early ~ responder,labeller = label_both)+
  theme_bw()+
  ggsci::scale_color_aaas()

g
```

## Simulating the Network

::: callout-note
## Interpreting profile of a non-responder

This is for a non-responder Note ATE as measured for counter-factual of $X_1^{\textrm{counterfactural}}$ relative to $X_1^{\textrm{obs}}$ is good but RXN still takes place in relatively low $X_1^{\textrm{obs}}$

:::

```{r, echo=F}
id_example <- sim_xbar_long %>% filter(responder == "N") %>%
  group_by(ID) %>% summarise(ntrt = sum(trt)) %>% 
    arrange(desc(ntrt)) %>%
    slice(1) %>% pull(ID)

scale <- 5
sim_xbar_long %>% ggplot(aes(x = t, y=x1, text = ID))+
  geom_line(aes(group = ID), color = "lightgray") +
  geom_line(data = sim_xbar_long %>% filter(ID == id_example), color = "blue") +
  geom_line(data = sim_xbar_long %>% filter(ID == id_example), aes(y = x2bar), 
            color = "blue", linetype = 3) +
  # annotate(geom = "text", data = sim_xbar_long %>% filter(ID == id_example, rxn ==  1),
  #        label = "RXN", color = "red") +
  
  geom_label(data = sim_xbar_long %>% filter(ID == id_example, rxn ==  1),
         label = "RXN", color = "red", alpha = .5) +

  geom_vline(data = sim_xbar_long %>% filter(ID == id_example, trt == 1),
             aes(xintercept = t), linetype = 2) +
  geom_line(data = sim_xbar_long %>% filter(ID == id_example),
            aes(y = pk/scale), color = "black")+
  scale_y_continuous(sec.axis = sec_axis(~.*scale, name="pk(t)")) +
  ylab(expression(X[1](t)))+
  ggtitle("Example of evolution of X1, counterfactial X1 and pk",
          subtitle = glue::glue("ID: {id_example}")) +
  theme_bw()

```

## Background and Notation

::: columns
::: {.column width="50%"}
Network: Nodes + Edges

![](images/net_bg_notation.png)
:::

::: {.column width="50%"}
Adjacency matrix: not-directed

$$A_{0/1} = \begin{bmatrix}
0 & 1 & 1 & 0  \\
1 & 0 & 1 & 0  \\
1 & 1 & 0 & 1  \\
0 & 0 & 1 & 0  \\
\end{bmatrix}$$

Adjacency matrix: directed $$A = \begin{bmatrix}
0.0 & 0.5 & 2.0 & 0.0  \\
0.0 & 0.0 & 1.0 & 0.0  \\
0.0 & 0.0 & 0.0 & 0.0  \\
0.0 & 0.0 & -1.0 & 0.0  \\
\end{bmatrix}$$
:::
:::

## Background and Notation

::: {style="font-size:24px"}
**Data representation:**

-   $X^t$: Observations (data) at time $t$
-   $X^t \in R^{nxp}$: $X$ contains $n$ observations on $p$ nodes at time $t$

::: callout-tip
## Conditional Independence

::: {style="font-size:24px"}
```{mermaid}
flowchart LR
  x1((A)) --> x2((B))
  x2 --> x3((C))
```

For this simple graph, conditional independence can be stated as

$$C \not\!\perp\!\!\!\perp A | B$$

As the number of nodes, $p$, grows, establishing conditional independence globally becomes challenging and many network inference algorithms rely on some variation of pairwise scoring between nodes, which is much more tractable. However, those algorithms are more prone to false edge discovery.
:::
:::
:::

## Conditional Independence Through Regression

::: {style="font-size:24px"}
```{mermaid}
flowchart LR
  x1t1((x1_t1)) --> x2t2((x2_t2))
  x2t2 --> x3t3((x3_t3))
```

::: columns
::: {.column width="50%"}
```{r, echo = T, fig.height= 5, fig.width=6}
xx <- sim_xbar_long %>% select(ID, t, x1) %>% 
    tidyr::pivot_wider(names_from = t,values_from = x1, names_prefix = "t_") 

xx %>% select(t_1, t_2, t_3) %>% plot
```
:::

::: {.column width="50%"}
```{r, echo=TRUE, fig.height= 6, fig.width=6}
mod <- lm(t_3 ~ t_2, data = xx)
plot(mod$residuals, xx$t_1)
```
:::
:::
:::

## Vector Autoregressive (VAR) Models

::: callout-tip
## Network VAR Model: General Idea

States observed over the past $d$ time points are predictive of the current observed state, $X^T$
:::

::: {style="font-size:24px"}
-   Example: a network with $p=5$ and $d = 2$ ![VAR Example](images/var_toy.png)

-   Task: estimate $d$ and $A^t$ given $X^t$, $t \in \{1, ..., T\}$

$$X^T = X^{T-1}A^1 + ... + X^{T-d}A^d + \epsilon^T, ~ \epsilon^T \sim 
\mathcal{MN}(0_{nxp},\sigma^2 I_n, \sigma^2 I_p) ~~ (1)$$
:::

## Estimating $A^t$s and $d$

-   Combining $X^T \rightarrow \mathcal{X}$ and stacking $A^t \rightarrow \mathcal{A}$:

$$X^T = \mathcal{X} \mathcal{A} + \epsilon^T, ~\epsilon^T \sim 
\mathcal{MN}(0_{nxp},\sigma^2 I_n, \sigma^2 I_p) ~~ (2)$$

-   Estimating $A^t$s amounts to regressing columns of $X^T$ on $\mathcal{X}$ one at a time

-   Observing that

  1. $\mathcal{X}$ is high dimensional ($n \ll p$) and that

  2. $A^t$ are expected to be sparse

  regression involves both variable **selection** (which nodes are connected) and **estimation** (quantification of connection strength). To this end, we can impose a $\ell_1$ type penalty to the regression. To estimate $d$ we could then use thresholded LASSO (Shojaie *et al*).

## Estimating $A^t$s and $d$ Using Thresholded LASSO

::: {style="font-size:24px"}
A 3-Step process for estimating $A^t$s and d by

1.  Estimate $\bar{A^t}$ by fit eq. 2 for each column $j$ of $X^T$

$$\underset{{\alpha_0 \in \mathbb{R}^{1},\mathcal{A}_{.j} \in \mathbb{R}^{q} }}{\arg\min} n^{-1}
\| X_{.j}^T  - \alpha_0 - \mathcal{X}\mathcal{A}_{.j}  \|_2^2
+ \lambda \|\mathcal{A}_{.j}\|_1~~(3)$$

2.  Threshold $\bar{A^t}$ to get the thresholded est. adj matrix, $\hat{A^t}$

$$\hat{A_{ij}^t} = \bar{A_{ij}^t} 1_{\{ \|\bar{A}^t\|_0 < \frac{p^2 \beta}{T-1} \text{ and }  |\bar{A_{ij}^t}| < \tau \}}~~(4)$$

3.  Estimate the order of the VAR model, $\hat{d}$, as follows:

$$\hat{d} = \max_{t} \{t: \|\hat{A}^{T-t}\|_0 < \frac{p^2 \beta}{T-1} \}~~(5)$$ where $\beta$ is the desired rate of type II error for detecting an edge between two nodes. $\lambda$ and $\tau$ are tuning parameters with default values suggested by Shojaie et al.
:::

## Estimating $A^t$s and $d$: A Toy Example

::: {style="font-size:24px"}
The 3-steps procedure for a network of size 3 observed over 3 time-points.

1.  Get the adj. mat. estimates $[\bar{A^1}~ \bar{A^2}~\bar{A^3}]$ via LASSO regression:

$$\begin{bmatrix}
0.00 & 0.00 & 1.00 ~~&~~ 1.00 & 0.00 & 1.00 ~~&~~ 0.10 & 2.00 & 0.90\\
0.00 & 0.00 & 0.00 ~~&~~ 0.00 & 5.00 & 0.00 ~~&~~ 0.00 & 0.00 & 0.00\\
0.00 & 0.00 & 0.00 ~~&~~ 0.35 & 0.32 & 0.09 ~~&~~ 5.00 & 0.20 & 0.15\\
\end{bmatrix}$$

2.  Threshold using eq. 4 with $\frac{p^2 \beta}{T-1} = 2$ and $\tau = 1$

$$\begin{bmatrix}
0 & 0 & 0 ~~&~~ 1 & 0 & 1 ~~&~~ 0 & 2 & 0\\
0 & 0 & 0 ~~&~~ 0 & 5 & 0 ~~&~~ 0 & 0 & 0\\
0 & 0 & 0 ~~&~~ 0 & 0 & 0 ~~&~~ 5 & 0 & 0\\
\end{bmatrix}$$

3.  Estimate the order: $\hat{d} = \max_{t} \{t: \|\hat{A}^t\|_0 < 2 \} = 2$
:::

## Generalizing the Model: Capturing Higher Order Relations

::: {style="font-size:24px"}
-   VAR model of eq. 1 estimates the linear trends in the inter-relations of nodes
-   To capture higher order relations, we transform observations using a smoother (e.g. B-spline)

$$X^T = Y^{T-1}B^1 + \cdots + Y^{T-d}B^d +\epsilon^T, \epsilon^T \sim \mathcal{MN}(\bf{0}_{\textrm{n} \times \textrm{p}},~\sigma^2I_n ,\sigma^2I_p)~~(6) $$ Here, $Y^t = [f(X_{.1}^t), \cdots, f(X_{.p}^t)]\in \mathbb{R}^{\textrm{n}\times \textrm{u}}$

-   We penalize the coefficients by groups corresponding to expansion of a single variable using group LASSO

$$\underset{\beta_0 \in \mathbb{R},\mathcal{B}_{.j} \in \mathbb{R}^{s} } {\arg\min}  \frac{1}{2n}
\| X_{.j}^T  - \beta_0 - \mathcal{Y} \mathcal{B}_{.j}  \|_2^2
+ \lambda \sum\limits_{i=1}^p \sqrt{k} \|\mathcal{B}_{g_ij}\|_1, ~~ (7)$$

-   Thresholding is done analogous to the linear case but using $\ell^2$ norm of est. coefficient groups.
:::

## Model Tuning: Regression Penalty Term $\lambda$

::: {style="font-size:24px"}
-   Network inference: filter out spurious associations, keep true conditional dependencies

-   the larger the penalty, $\lambda$, the more conservative the filter

-   To pick the right level of filtering, there are multiple choices including general cross validation as well as likelihood-based scores, such as Schwarz Bayesian Information Criteria (SBIC) score.

$$\textrm{SBIC} = \: \log \: \mathcal{L}(\theta_{MLE}|x) - \frac{DOF_{MLE}}{2} \: \log \: n ~~ (8)$$   where DOF is the degrees of freedom of MLE model: $\lVert\theta_{MLE}\rVert_0$

-   In the context of linear regression model, $lm$, and assumption of normality, BIC (which is often represented as SBIC scaled by -2) is simplified (up to a constant) as:

$$\textrm{BIC} = \: n \: \log \: \frac{RSS}{n} + DOF_{lm} \: \log \: n ~~ (9)$$
:::

## Model Tuning: BIC's flat Prior Assumption

::: {style="font-size:24px"}
-   In high-dimensional settings, BIC is too liberal in model selection

-   Flat Prior Assumption of Schwarz's BIC: all possible models equally likely given that we have no information on the network

-   Consider the case where there are 1000 parameters (edges) that could be included in the model (network).

    -   1 param model choices: $\binom{1000}{1} = 1000$
    -   2 param model choices: $\binom{1000}{2} = 1000 * 999/2$
    -   $\rightarrow$ two parameter model are 999/2 more likely than single parameter model

-   Extended BIC, proposed by Chen and Chen, sets the prior to be flat for model groups of same number of parameters.
:::

## Model Tuning: Extended BIC with $\widehat{\textrm{DOF}}_{\textrm{grplasso}}$

::: {style="font-size:24px"}
-   Extended BIC (EBIC) is given by

$$\textrm{EBIC} = \textrm{BIC} + 2~\textrm{DOF}~\gamma~ log~p,~ \text{where}~  0\leq \gamma ~~(10)$$

|       DOF is the degrees of freedom and $\gamma$ is a tuning parameter that can be increased as p grows larger
|       relative to n. At $\gamma = 0$, EBIC reduces to BIC

-   Note: DOF relates to the number of model parameter. However, the true number of parameters (edges) to estimated itself is unknown $\rightarrow$ We need an estimator for DOF.

-   For group LASSO, we can use Vaiter et al. est. DOF

$$\widehat{\textrm{DOF}}_{\textrm{grplasso}} =  \textrm{Trace}[X_a(X_a'X_a +
\lambda \mathscr{N}(\hat{\beta_a})\odot
(I - P_{\hat{\beta_a}}))^{-1}X_a'] ~~(11)$$

-   where $\lambda$ is the group LASSO penalty, $X_a$ and $\hat{\beta_a}$ correspond to 'selected\` groups of variables. $P_{\hat{\beta_a}}$ is a block diagonal matrix, each block the projection matrix of one of the 'selected' variables. $\mathscr{N}(\hat{\beta_a})$ normalizes each block by its $\ell^2$ norm of $\hat{\beta}_{\textrm{group}}$.
:::

## Catching the 'Signal' in the Time Course

-   Conditional dependencies (edges) between nodes are learned from observing nonde-node co-variations

-   As a signal propagates through network, different sets of nodes exhibit their co-variations maximally at different time windows

-   During which time window should these co-variations be estimated?

![](images/catching_the_signal.png){fig-align="center"}

## Bayesian Model Averaging (BMA)

::: {style="font-size:24px"}
-   For $\forall~\mathcal{X}_t = [X^{1}, ..., X^{t}]$, where $t \in \{2,~...,~ T\}$ infer network: $M_t$ and the corresponding graph $G_t$

-   Score each inferred network, $M_t$, using BIC [^1],

$$w_i = \frac{\exp(-\frac{1}{2} \textrm{BIC}_{i})}{\sum\limits_{j=1}^{T-1}  
\exp(-\frac{1}{2} \textrm{BIC}_j) } ~~(12)$$

-   Perform a weighted average of the est. graph

$$\hat{\mathcal{G}} = \sum\limits_{t=1}^{T-1}w_t{\hat{G}_t} ~~(13)$$

-   Select a thresholding $\mathcal{p} \in [0~1]$ and define thresholded BMA graph , $\breve{\mathcal{G}}$ such that: $\breve{\mathcal{G}}_{ij} = \hat{\mathcal{G}_{ij}} 1_{[\hat{\mathcal{G}_{ij}} > \mathcal{p}]}$
:::

[^1]: BIC is used generically to refer to EBIC or BIC

## Summary and Future Work

::: {style="font-size:24px"}
The framework described has the following favorable properties

-   With some mild assumptions, it can infer network topology based on conditional independence

-   It is well-suited for high-dimensional data, assuming sparsity of network

-   It can handle higher order trajectories

-   It expands to enable analysis of simultaneously evolving processes (e.g. multiple biomarkers)

-   It can handle experimental perturbation of nodes

-   It relies on well-established and computationally efficient regression machinery

### Feature work

-   Quantifying algorithm efficiency using simulated data

-   Expansion to handle censored data

-   Computational parallelization

-   Total regression (as opposed to least squares)
:::
